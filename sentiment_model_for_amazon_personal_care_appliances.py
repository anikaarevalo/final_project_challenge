# -*- coding: utf-8 -*-
"""Sentiment_Model_for_Amazon_Personal_Care_Appliances.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fmsdYAjKmqz2XFEQDV0ZVDAdp1-LuIoN

## IMPLEMENTATION

### I. Installing libraries and dependencies
"""

import tensorflow as tf

num_gpus_available = len(tf.config.experimental.list_physical_devices('GPU'))
print("Num GPUs Available: ", num_gpus_available)  # Note that the model will require the use of GPU
assert num_gpus_available > 0

!pip install transformers

from transformers import DistilBertTokenizerFast
from transformers import TFDistilBertForSequenceClassification
import pandas as pd
import numpy as np

import nltk
import re
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

"""### II. Importing data set and pre-processing

The dataset that I am using is the **amazon personal care appliances reviews** which is a subset of the large Amazon Product review. The dataset is already stored in the **TensorFlow database** and can be loaded directly using the ‘tfds‘ API from Tensorflow.
"""

import tensorflow_datasets as tfds
ds = tfds.load('amazon_us_reviews/Personal_Care_Appliances_v1_00', split='train', shuffle_files=True)
assert isinstance(ds, tf.data.Dataset)
print(ds)

# Converting dataset into a pandas data frame using ‘tfds.as_dataframe‘ API.
df = tfds.as_dataframe(ds)

# Getting a glimpse of the data frame
df.head()

"""**BINARY CLASSIFCATION**

The **rating provided by the customer is on a scale of 1-5**( 5 being the highest). As I am going to implement a *binary classification model*, I shall be converting these ratings into 2 categories, i.e 1 and 0. **Ratings above and equal to 3 will be labeled as Positive(1)** and **below 3 will be negative(0)**. The following code will help us implement these steps.
"""

df["Sentiment"] = df["data/star_rating"].apply(lambda score: "positive" if score >= 3 else "negative")
df['Sentiment'] = df['Sentiment'].map({'positive':1, 'negative':0})

df['short_review'] =df['data/review_body'].str.decode("utf-8")

df = df[["short_review", "Sentiment"]]

"""**Pre-trained BERT NLP model and FEATURE SELECTION**

The dataset consists of several columns ranging from Product ID to reviews, heading, and star rating provided by the customer. For this project, I am extracting mainly the **reviews** (ie feature column) and the **corresponding rating provided by the customer** (i.e. label column). Hence, I am dropping the other features/columns in this data frame. 

Since our chosen pre-trained model, BERT, requires a lot of computational power and a huge corpus of training data for model development, I am only going to focus on the said two columns of the data frame. 
"""

# Dropping last n rows using drop
n = 54975
df.drop(df.tail(n).index,
        inplace = True)

index = df.index
number_of_rows = len(index)
print(number_of_rows)

# A glimpse of the tail-end of the data frame
df.tail()

# Another glimpse of the data frame from the top
df.head()

"""### III. Tokenization of text and conversion into tokens

There is a need *to convert the review column into numerical values* as machine learning models operate on numerical features. This is called **text vectorisation**. 

**METHOD**

In this application, I am going to use the **Tokenizer class from pre-trained DistilBert.**
"""

# Converting the feature column ('short_review') and label ('Sentiment') into a set of lists as that’s how our Tokenizer wants the data. 
reviews = df['short_review'].values.tolist()
labels = df['Sentiment'].tolist()

print(reviews[:2])
print(labels[:2])

"""**SPLITTING DATA INTO TRAINING AND TESTING SETS**

The tokenizer needs to be fit on the training set. To split the data into training and validation sets, we will make use of Train-test-split class from Scikit-Learn.
"""

from sklearn.model_selection import train_test_split
training_sentences, validation_sentences, training_labels, validation_labels = train_test_split(reviews, labels, test_size=.2)

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

tokenizer([training_sentences[0]], truncation=True,
                            padding=True, max_length=128)

"""The **Tensorflow API** provides a seemingly easy way to build data pipelines. Using 'from-Tensor-Slices', we can easily combine our features tokens and labels into a dataset."""

train_encodings = tokenizer(training_sentences,
                            truncation=True,
                            padding=True)
val_encodings = tokenizer(validation_sentences,
                            truncation=True,
                            padding=True)

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    training_labels
))

val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    validation_labels
))

"""## MODEL TRAINING AND OPTIMISATION

In this application, I am going to use **TFDistilBertForSequenceClassification** for the sentiment analysis and put the **‘num-labels’ parameter equal to 2 as we are doing a binary classification**.
"""

model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=2)

# Using the Hugginface model saves as the time and effor to build the model on one's own
# https://www.tensorflow.org/official_models/fine_tuning_bert_files/output_8L__-erBwLIQ_0.png?dcb_=0.04391390122987171

model.summary()

"""There is no need to put additional layers; and using **a Hugging face transformer**, the model can now be trained with the following configuration:

**Epochs**: 2
**Batch size:** 16
**Learning rate (Adam):** 5e-5 (0.00005)

*If the number of epochs  increased, it will give rise to overfitting problems as well as take more time for the model to train*. 
"""

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-08)
model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])
model.fit(train_dataset.shuffle(100).batch(16),
          epochs=2,
          batch_size=16,
          validation_data=val_dataset.shuffle(100).batch(16))

"""*The complete model gets trained in around 2 hours, that’s why it is important to keep the number of epochs and batch size low.*

**In just 2 epochs, the models give 94.73% and 92% accuracy on the Training and Validation sets, respectively. Once the model is trained, it can be saved then loaded later for model evaluation on unseen data. This can be done using ‘model.save’ API**

**SAVING THE TRAINED MODEL**
"""

model.save_pretrained("./sentiment")

loaded_model = TFDistilBertForSequenceClassification.from_pretrained("./sentiment")

"""## EVALUATION

To evaluate the model accuracy on unseen data, the saved data can be loaded and **tested on new input (sentences) to see if the sentiment is predicted correctly or not**.
"""

test_sentence = "This is a really good product. I love it"


predict_input = tokenizer.encode(test_sentence,
                                 truncation=True,
                                 padding=True,
                                 return_tensors="tf")

tf_output = loaded_model.predict(predict_input)[0]


tf_prediction = tf.nn.softmax(tf_output, axis=1)
labels = ['Negative','Positive']
label = tf.argmax(tf_prediction, axis=1)
label = label.numpy()
print(labels[label[0]])

"""**This returns ‘Positive’. Having tried several other types of sentences, the model performed fairly well in predicting the sentiment of the sentences.**

## CONCLUSION
"""